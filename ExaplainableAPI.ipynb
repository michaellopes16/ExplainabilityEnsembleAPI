{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AdaQttrzx8_6",
        "aygJH5C7_KcT",
        "jiuxnZsa_GDK",
        "2SXH8jT__Qph",
        "am70VoG-Ptch"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaellopes16/ExplainabilityEnsembleAPI/blob/main/ExaplainableAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install Libs**\n",
        "\n"
      ],
      "metadata": {
        "id": "AdaQttrzx8_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !jupyter nbconvert --to script ExplainableAPI.ipynb\n"
      ],
      "metadata": {
        "id": "76I9ZKEjPQ6e"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime==0.1.1.37"
      ],
      "metadata": {
        "id": "3YGmvoFh7qXn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3391fe87-6054-4f2a-cd32-9fb6f434626e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lime==0.1.1.37 in /usr/local/lib/python3.10/dist-packages (0.1.1.37)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lime==0.1.1.37) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime==0.1.1.37) (1.13.1)\n",
            "Requirement already satisfied: progressbar in /usr/local/lib/python3.10/dist-packages (from lime==0.1.1.37) (2.5)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime==0.1.1.37) (1.3.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime==0.1.1.37) (3.7.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime==0.1.1.37) (0.23.2)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime==0.1.1.37) (3.3)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime==0.1.1.37) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime==0.1.1.37) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime==0.1.1.37) (2024.8.28)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime==0.1.1.37) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime==0.1.1.37) (0.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime==0.1.1.37) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime==0.1.1.37) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime==0.1.1.37) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime==0.1.1.37) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime==0.1.1.37) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime==0.1.1.37) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime==0.1.1.37) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime==0.1.1.37) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lime==0.1.1.37) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap==0.46.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68ELXSZA_lED",
        "outputId": "d807f087-5ba0-4604-a155-98ac65654a7a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap==0.46.0 in /usr/local/lib/python3.10/dist-packages (0.46.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (1.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (2.1.4)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (4.66.5)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (24.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (0.0.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap==0.46.0) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap==0.46.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap==0.46.0) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap==0.46.0) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap==0.46.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap==0.46.0) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap==0.46.0) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTS**"
      ],
      "metadata": {
        "id": "aygJH5C7_KcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crie um classe em python chamada ExplainableAPI com o m√©todo load_data que recebe o caminho, o separador e retorna um x e um y\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "# from multipledispatch import dispatch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import lime.lime_tabular # !pip install lime==0.1.1.37\n",
        "import shap #!pip install shap==0.46.0"
      ],
      "metadata": {
        "id": "I8Cfdiu97kAd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GENERAL API**"
      ],
      "metadata": {
        "id": "pdStW9-5_MkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExplainableAPI:\n",
        "  QUERY_FUNGI = '''SELECT\n",
        "            f.name AS Fungi_Name,\n",
        "            v.VOC_Category,\n",
        "            COUNT(*) AS VOC_Count\n",
        "        FROM\n",
        "            FUNGI f\n",
        "        JOIN\n",
        "            FUNGI_VOC fv ON f.id = fv.FUNGI_ID\n",
        "        JOIN\n",
        "            VOCs v ON fv.VOC_ID = v.id_VOC\n",
        "        GROUP BY\n",
        "            f.name, v.VOC_Category\n",
        "        ORDER BY\n",
        "            f.name, v.VOC_Category,VOC_Count;\n",
        "        '''\n",
        "  QUERY_SENSOR = '''SELECT\n",
        "            f.name AS Sensor_Name,\n",
        "            v.VOC_Category,\n",
        "            COUNT(*) AS VOC_Count\n",
        "        FROM\n",
        "            SENSOR f\n",
        "        JOIN\n",
        "            SENSOR_VOC fv ON f.id = fv.SENSOR_ID\n",
        "        JOIN\n",
        "            VOCs v ON fv.VOC_ID = v.id_VOC\n",
        "        GROUP BY\n",
        "            f.name, v.VOC_Category\n",
        "        ORDER BY\n",
        "            f.name,VOC_Count, v.VOC_Category;\n",
        "        '''\n",
        "  def __init__(self, features_name):\n",
        "    self.features_name = features_name\n",
        "  def load_data(self, path, sep):\n",
        "      data = pd.read_csv(path, delimiter=sep,header=None)\n",
        "      print()\n",
        "      y = data.iloc[:, data.shape[1]-1].values\n",
        "      x = data.iloc[:,0: data.shape[1]-1].copy().values\n",
        "      return x, y\n",
        "\n",
        "  def create_df_2(self, list_weights):\n",
        "      df = pd.DataFrame({\n",
        "      'Feature': range(len(list_weights)),\n",
        "      'Weight': list_weights\n",
        "      })\n",
        "      return df\n",
        "\n",
        "  def get_predicted_class(self, sample, model):\n",
        "      instancia = np.expand_dims(sample, axis=0)\n",
        "      result = model.predict(instancia)\n",
        "      return np.argmax(result[0]) +1\n",
        "\n",
        "  def create_df(self, list_weights, features):\n",
        "      df = pd.DataFrame({\n",
        "      'Feature': features,\n",
        "      'Weight': list_weights\n",
        "      })\n",
        "      return df\n",
        "      #melhorar isso\n",
        "  def get_samples_from_db(self, X_train, y_train, predictedClass):\n",
        "      # Filtrar as inst√¢ncias pela classe desejada\n",
        "      indices_classe = np.where(y_train == predictedClass)[0]\n",
        "      X_classe = X_train[indices_classe]\n",
        "      print(\"X_classe.shape[0]\")\n",
        "      print(X_classe.shape[0])\n",
        "      # Selecionar 3 exemplos aleat√≥rios das inst√¢ncias filtradas\n",
        "      amostras_aleatorias = X_classe[np.random.choice(X_classe.shape[0], 3, replace=False)]\n",
        "      # Converter cada exemplo selecionado em um DataFrame e armazenar em um array\n",
        "      dataframes = [pd.DataFrame(amostra.reshape(1, -1)) for amostra in amostras_aleatorias]\n",
        "      return dataframes\n",
        "\n",
        "  def plot_cycles(self,newDF):\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Plotando os dataframes em gr√°ficos separados\n",
        "    for i, df in enumerate(newDF):\n",
        "        fig = plt.figure(figsize=[20,10])\n",
        "        plt.plot(df.T, label=f'Ciclo {i+1}')\n",
        "        plt.title(f'Gr√°fico de Linha do Ciclo {i+1}')\n",
        "        plt.xlabel('√çndice')\n",
        "        plt.ylabel('Valores')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    # Mostrando o gr√°fico\n",
        "    plt.show()\n",
        "\n",
        "  def weight_by_feature(self, features_name, weight_list):\n",
        "      # Converte a lista de pesos em um array NumPy para facilitar a manipula√ß√£o\n",
        "      weight_array = np.array(weight_list)\n",
        "\n",
        "      # Calcula o n√∫mero de linhas que o DataFrame ter√°\n",
        "      num_rows = len(weight_list) // len(features_name)\n",
        "\n",
        "      # Redimensiona o array para ter 'num_rows' linhas e 'len(features_name)' colunas\n",
        "      reshaped_array = weight_array[:num_rows * len(features_name)].reshape(num_rows, len(features_name))\n",
        "\n",
        "      # Cria o DataFrame a partir do array redimensionado\n",
        "      df = pd.DataFrame(reshaped_array, columns=features_name)\n",
        "      return df\n",
        "\n",
        "  def get_final_result(self, wigths, feature):\n",
        "      df = self.create_df(wigths, feature)\n",
        "      self.ploat_heatmap(df)\n",
        "      self.ploat_bar(df)\n",
        "      df1 = self.weight_by_feature(self.features_name, df.Weight)\n",
        "      df1.max().plot(kind='bar')\n",
        "      df1.mean().plot(kind='bar')\n",
        "      return df1\n",
        "\n",
        "  def get_most_important_features(self, dataFrame, top_feature_number):\n",
        "\n",
        "      df = self.weight_by_feature(self.features_name, dataFrame.Weight)\n",
        "\n",
        "      newDf = df.mean().reset_index()\n",
        "      newDf.columns = ['Column', 'Mean']\n",
        "      newDf = newDf.sort_values(by='Mean', ascending=False)\n",
        "      top_features = newDf.head(top_feature_number)\n",
        "      return top_features\n",
        "\n",
        "  def concat_all_mothods(self, means_lime, means_shap, means_grad):\n",
        "      from sklearn.preprocessing import StandardScaler\n",
        "      scaler = StandardScaler()\n",
        "      # Calculando as m√©dias das colunas\n",
        "      means_lime['Mean'] = scaler.fit_transform(means_lime[['Mean']])\n",
        "      means_shap['Mean'] = scaler.fit_transform(means_shap[['Mean']])\n",
        "      means_grad['Mean'] = scaler.fit_transform(means_grad[['Mean']])\n",
        "\n",
        "      # # Renomeando as colunas para facilitar a concatena√ß√£o\n",
        "      # means_lime.columns = ['Column', 'Mean']\n",
        "      # means_shap.columns = ['Column', 'Mean']\n",
        "      # means_grad.columns = ['Column', 'Mean']\n",
        "    #   print(\"means_grad\")\n",
        "    #   print(means_grad)\n",
        "      # Adicionando uma coluna para identificar o dataframe\n",
        "      means_lime['DataFrame'] = 'LIME'\n",
        "      means_shap['DataFrame'] = 'SHAP'\n",
        "      means_grad['DataFrame'] = 'GRAD'\n",
        "\n",
        "      # Concatenando os dataframes\n",
        "      means_all = pd.concat([means_lime, means_shap, means_grad])\n",
        "      print(means_all)\n",
        "      # means_all['Mean'] = scaler.fit_transform(means_all[['Mean']])\n",
        "    #   self.plot_bar_chart_all_methods(means_all)\n",
        "      return means_all, means_lime, means_shap, means_grad\n",
        "\n",
        "  def get_top_features(self,means_lime,means_shap,means_grad,top_index ):\n",
        "      df_sorted_1 = means_lime.sort_values(by='Mean', ascending=False)\n",
        "      df_sorted_2 = means_shap.sort_values(by='Mean', ascending=False)\n",
        "      df_sorted_3 = means_grad.sort_values(by='Mean', ascending=False)\n",
        "\n",
        "      # Selecionar as tr√™s primeiras linhas\n",
        "      top_3_1 = df_sorted_1.head(top_index)\n",
        "      top_3_2 = df_sorted_2.head(top_index)\n",
        "      top_3_3 = df_sorted_3.head(top_index)\n",
        "\n",
        "      return top_3_1, top_3_2, top_3_3\n",
        "\n",
        "  def get_features_in_common(self, df1, df2, df3):\n",
        "      means_all, means_lime, means_shap, means_grad = self.concat_all_mothods(df1, df2, df3)\n",
        "      top_3_1, top_3_2, top_3_3 = self.get_top_features(means_lime, means_shap, means_grad, 3 )\n",
        "      # Adicionar uma coluna de peso a cada DataFrame. Para remover os pesos, comentar essas linhas e a de m√©dia ponderada com o [X]\n",
        "      # top_3_1['Peso'] = 0.25\n",
        "      # top_3_2['Peso'] = 0.35\n",
        "      # top_3_3['Peso'] = 0.40\n",
        "\n",
        "      df_concat = pd.concat([top_3_1, top_3_2, top_3_3])\n",
        "      # Contar a frequ√™ncia de cada valor na coluna 'Coluna'\n",
        "      coluna_counts = df_concat['Column'].value_counts()\n",
        "      # Filtrar os valores que aparecem mais de uma vez\n",
        "      repeated_sensors = coluna_counts[coluna_counts > 1].index\n",
        "      # Criar um novo DataFrame com as linhas que t√™m 'Coluna' repetida\n",
        "      df_final = df_concat[df_concat['Column'].isin(repeated_sensors)]\n",
        "\n",
        "      # Calcular a m√©dia ponderada para cada valor na coluna 'Coluna' [X]\n",
        "      # df_final = df_final.groupby('Column').apply(lambda x: (x['Peso'] * x.drop(columns=['Peso'])).sum() / x['Peso'].sum())\n",
        "\n",
        "      df_final = df_final.sort_values(by='Column')\n",
        "      return df_final\n",
        "  def get_sensor_repeats(self, df_final):\n",
        "      coluna_counts = df_final['Column'].value_counts()\n",
        "      df_counts = pd.DataFrame({\n",
        "          'Sensors': coluna_counts.index,\n",
        "          'Repeats': coluna_counts.values\n",
        "      })\n",
        "      df_counts = df_counts.sort_values(by='Sensors').reset_index(drop=True)\n",
        "      return df_counts\n",
        "\n",
        "  def get_dict_by_query(self, query_sql, conn):\n",
        "      cursor = conn.cursor()\n",
        "      cursor.execute(query_sql)\n",
        "      conn.commit()\n",
        "      result = cursor.fetchall()\n",
        "      result_dict = {}\n",
        "      for data, category, count in result:\n",
        "          if data not in result_dict:\n",
        "              result_dict[data] = {}\n",
        "          result_dict[data][category] = count\n",
        "      return result_dict, result\n",
        "\n",
        "  # Fun√ß√£o para verificar compatibilidade e retornar os 3 fungos mais compat√≠veis\n",
        "  def find_top_compatible_fungi(self, sensor,fungi_dict,sensor_dict ):\n",
        "      sensor_categories = sensor_dict.get(sensor, {})\n",
        "      compatibility = []\n",
        "\n",
        "      for fungus, categories in fungi_dict.items():\n",
        "          common_categories = set(sensor_categories.keys()).intersection(categories.keys())\n",
        "          if len(common_categories) >= 1:\n",
        "              common_details = {category: (sensor_categories[category], categories[category]) for category in common_categories}\n",
        "              compatibility.append((fungus, len(common_categories), common_details))\n",
        "\n",
        "      # Ordenar por n√∫mero de categorias compat√≠veis (maior para menor)\n",
        "      compatibility.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "      # Ordenar as categorias compat√≠veis por quantidade de repeti√ß√µes (maior para menor)\n",
        "      for i in range(len(compatibility)):\n",
        "          compatibility[i] = (compatibility[i][0], compatibility[i][1], dict(sorted(compatibility[i][2].items(), key=lambda item: item[1][1], reverse=True)))\n",
        "\n",
        "      # Retornar os 3 fungos mais compat√≠veis\n",
        "      return compatibility[:3]\n",
        "\n",
        "  def plot_samples_db(self, data):\n",
        "    for i, cycle in enumerate(data):\n",
        "        a = np.array(cycle)[0]\n",
        "        new_df = self.group_sensor(features_name, a)\n",
        "        self.plot_chart_line_df(new_df)\n",
        "\n",
        "  def group_sensor(self, features_name, row_cycle):\n",
        "        # Converte a lista de pesos em um array NumPy para facilitar a manipula√ß√£o\n",
        "        weight_array = np.array(row_cycle)\n",
        "        # Calcula o n√∫mero de linhas que o DataFrame ter√°\n",
        "        num_rows = len(row_cycle) // len(features_name)\n",
        "        # print(len(row_cycle))\n",
        "        # print(len(features_name))\n",
        "        # print(num_rows)\n",
        "        # Redimensiona o array para ter 'num_rows' linhas e 'len(features_name)' colunas\n",
        "        reshaped_array = weight_array[:num_rows * len(features_name)].reshape(num_rows, len(features_name))\n",
        "\n",
        "        # Cria o DataFrame a partir do array redimensionado\n",
        "        df = pd.DataFrame(reshaped_array, columns=features_name)\n",
        "\n",
        "        return df\n",
        "  # Adicionando uma coluna de √≠ndice para o eixo X\n",
        "  def plot_chart_line_df(self, dataFrame):\n",
        "      dataFrame['Index'] = dataFrame.index\n",
        "      # Transformando o DataFrame para o formato longo\n",
        "      df_long = pd.melt(dataFrame, id_vars=['Index'], var_name='Sensor', value_name='Valor')\n",
        "      # Plotando o gr√°fico de linha\n",
        "      plt.figure(figsize=(14, 8))\n",
        "      sns.lineplot(data=df_long, x='Index', y='Valor', hue='Sensor')\n",
        "      plt.xlabel('Index')\n",
        "      plt.ylabel('Value')\n",
        "      plt.title('Line chart sensors')\n",
        "      plt.legend(title='Sensors')\n",
        "      plt.show()\n",
        "\n",
        "  def ploat_heatmap(self, dataFrame):\n",
        "      fig = px.density_heatmap(dataFrame, x='Feature', y='Weight', nbinsx=20, nbinsy=20, color_continuous_scale='Viridis')\n",
        "      # Atualizar o layout para permitir zoom e melhor visualiza√ß√£o\n",
        "      fig.update_layout(\n",
        "          title='Dynamic Heatmap of Feature Weightss',\n",
        "          xaxis=dict(title='Feature'),\n",
        "          yaxis=dict(title='Weight'),\n",
        "          autosize=True\n",
        "      )\n",
        "      # Mostrar o mapa de calor\n",
        "      fig.show()\n",
        "\n",
        "  def ploat_bar(self, dataFrame):\n",
        "      fig = px.bar(dataFrame, x='Feature', y='Weight', color='Weight', color_continuous_scale='Viridis')\n",
        "      # Atualizar o layout para permitir zoom e melhor visualiza√ß√£o\n",
        "      fig.update_layout(\n",
        "          title='Feature Weights Bar Chart',\n",
        "          xaxis=dict(title='Feature'),\n",
        "          yaxis=dict(title='Weight'),\n",
        "          autosize=True\n",
        "      )\n",
        "      # Mostrar o gr√°fico de barras\n",
        "      fig.show()"
      ],
      "metadata": {
        "id": "HvN5VPYyyLmR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LIME METHOD**"
      ],
      "metadata": {
        "id": "jiuxnZsa_GDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LIME_Method:\n",
        "  def __init__(self, api):\n",
        "    self.api = api\n",
        "    pass\n",
        "  def run_LIME_Method(self, train_data, test_data,sample_index, model, class_names):\n",
        "    print(\"Running LIME...\")\n",
        "    feature_names = np.array(range(train_data.shape[1]))\n",
        "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "        training_data=train_data,\n",
        "        feature_names=feature_names,\n",
        "        class_names=class_names,\n",
        "        discretize_continuous=False  # ou True, dependendo da natureza dos seus dados\n",
        "    )\n",
        "    num_features = train_data.shape[1]\n",
        "    instance = test_data[sample_index]\n",
        "    explanation = explainer.explain_instance(\n",
        "        instance,\n",
        "        model.predict,\n",
        "        num_features= num_features\n",
        "    )\n",
        "    # Print the explanation\n",
        "    # for feature, weight in explanation.as_list():\n",
        "    #     print(f'{feature}: {weight}')\n",
        "    feature , weigths = zip(*sorted(explanation.as_list()))\n",
        "    weigths_array = np.array(weigths)\n",
        "    # weigths_matriz = weigths_array.reshape(1, -1)  # Redimensiona para 1 linha e 'n' colunas\n",
        "    return explanation, feature , weigths_array\n",
        "\n",
        "  def run_mulltiple_LIME_Method(self, train_data, test_data,sample_index, model, class_names,REPEATS):\n",
        "      print(\"Running LIME...\")\n",
        "      df_results = []\n",
        "      for i in range(REPEATS):\n",
        "          print(f\"Cicle: {i+1} of {REPEATS}...\")\n",
        "          explanation, index, wiegths = self.run_LIME_Method(train_data=train_data, test_data=test_data, sample_index=sample_index, model=model, class_names=class_names)\n",
        "          dataFrame = self.api.create_df(wiegths, index)\n",
        "          df = self.api.get_most_important_features(dataFrame, 3)\n",
        "          df_results.append(df)\n",
        "      return self.summarize_df(df_results)\n",
        "\n",
        "  def summarize_df(self, df_results):\n",
        "      #Colocar isso em um m√©todo\n",
        "      df_final = pd.concat(df_results, ignore_index=True)\n",
        "      # Contar a frequ√™ncia de cada valor na coluna 'Coluna'\n",
        "      coluna_counts = df_final['Column'].value_counts()\n",
        "\n",
        "      # Filtrar os valores que aparecem mais de uma vez\n",
        "      repeated_sensors = coluna_counts[coluna_counts > 1].index\n",
        "\n",
        "      # Criar um novo DataFrame com as linhas que t√™m 'Coluna' repetida\n",
        "      df_final = df_final[df_final['Column'].isin(repeated_sensors)]\n",
        "\n",
        "      # Calcular a m√©dia das colunas repetidas\n",
        "      df_final = df_final.groupby('Column').mean().reset_index()\n",
        "\n",
        "      df_final = df_final.sort_values(by='Column')\n",
        "      return df_final\n",
        "\n",
        "  def plot_bar_chart(self, df_final):\n",
        "      # Configurar o estilo do Seaborn\n",
        "      sns.set(style=\"whitegrid\")\n",
        "\n",
        "      # Criar o gr√°fico de barras\n",
        "      plt.figure(figsize=(12, 8))\n",
        "      barplot = sns.barplot(x='Column', y='Mean', hue='Column', data=df_final, palette='viridis')\n",
        "\n",
        "      # Adicionar t√≠tulos e r√≥tulos\n",
        "      plt.xlabel('Sensores')\n",
        "      plt.ylabel('M√©dia')\n",
        "      plt.title('M√©dia dos Sensores para o m√©todo LIME')\n",
        "\n",
        "      # Mostrar o gr√°fico\n",
        "      plt.show()\n",
        "\n",
        "  def plot_class_proba(self, class_name, explanation):\n",
        "      # Seus valores de probabilidade\n",
        "      probabilidades = explanation.predict_proba\n",
        "      # Nomes das classes para o eixo x do gr√°fico\n",
        "      plt.figure(figsize=(10, 7))  # Ajuste o tamanho conforme necess√°rio\n",
        "      # Criar o gr√°fico de barras\n",
        "      plt.bar(class_name, probabilidades)\n",
        "\n",
        "      # Adicionar t√≠tulo e r√≥tulos aos eixos\n",
        "      plt.title('Probabilidades das Classes')\n",
        "      plt.xlabel('Classes')\n",
        "      plt.ylabel('Probabilidade')\n",
        "\n",
        "      # Mostrar o gr√°fico\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "-3nY0q2l5-10"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df1 = api.get_final_result(wieghts,index)"
      ],
      "metadata": {
        "id": "awhbAXd0m2w_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SHAP METHOD**"
      ],
      "metadata": {
        "id": "2SXH8jT__Qph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SHAP_Method:\n",
        "  def __init__(self, features_name, api):\n",
        "    self.api = api\n",
        "    self.features_name = features_name\n",
        "  def run_SHAP_Method(self, train_data, test_data,sample_index, model, class_names, npermutations):\n",
        "    # explainer = shap.KernelExplainer(model.predict, train_data)\n",
        "    print(\"Running SHAP...\")\n",
        "    explainer = shap.PermutationExplainer(model.predict, train_data)\n",
        "    shap_values = explainer.shap_values(test_data[sample_index-1:sample_index], npermutations=npermutations)\n",
        "\n",
        "    predicted_classes, main_class_predicted = self.get_model_results(model, test_data,sample_index )\n",
        "\n",
        "    feature_weights = shap_values[:,:,main_class_predicted][0]  # Sum weights across classes #max_index\n",
        "    # culture_time_weigth = np.mean(feature_weights[0][train_data.shape[1]])\n",
        "    # mean_array = np.mean(feature_weights, axis=2)\n",
        "    # mean_array = mean_array[0][:-1]\n",
        "    # df2 = api.weight_by_feature(self.features_name, mean_array,culture_time_weigth)\n",
        "    return explainer,shap_values, feature_weights\n",
        "  def run_mulltiple_SHAP_Method(self, train_data, test_data,sample_index, model, class_names, npermutations, REPEATS):\n",
        "      print(\"Running SHAP...\")\n",
        "      df_results = []\n",
        "      for i in range(REPEATS):\n",
        "          print(f\"Cicle: {i+1} of {REPEATS}...\")\n",
        "          explanation, index, wieghts = self.run_SHAP_Method(train_data=train_data, test_data=test_data, sample_index=sample_index, model=model, class_names=class_names, npermutations=npermutations)\n",
        "          dataFrame = self.api.create_df_2(wieghts)\n",
        "          df = self.api.get_most_important_features(dataFrame, 3)\n",
        "          df_results.append(df)\n",
        "      return self.summarize_df(df_results)\n",
        "\n",
        "  def get_model_results(self, model, test_data, sample_index):\n",
        "      predicted_classes = model.predict(test_data[sample_index-1:sample_index])\n",
        "      main_class_predicted = np.argmax(predicted_classes)\n",
        "      return predicted_classes, main_class_predicted\n",
        "\n",
        "  def summarize_df(self, df_results):\n",
        "      #Colocar isso em um m√©todo\n",
        "      df_final = pd.concat(df_results, ignore_index=True)\n",
        "      # Contar a frequ√™ncia de cada valor na coluna 'Coluna'\n",
        "      coluna_counts = df_final['Column'].value_counts()\n",
        "\n",
        "      # Filtrar os valores que aparecem mais de uma vez\n",
        "      repeated_sensors = coluna_counts[coluna_counts > 1].index\n",
        "\n",
        "      # Criar um novo DataFrame com as linhas que t√™m 'Coluna' repetida\n",
        "      df_final = df_final[df_final['Column'].isin(repeated_sensors)]\n",
        "\n",
        "      df_final = df_final.sort_values(by='Column')\n",
        "      return df_final\n",
        "\n",
        "  def plot_bar_chart(self, df_final):\n",
        "      # Configurar o estilo do Seaborn\n",
        "      sns.set(style=\"whitegrid\")\n",
        "\n",
        "      # Criar o gr√°fico de barras\n",
        "      plt.figure(figsize=(12, 8))\n",
        "      barplot = sns.barplot(x='Column', y='Mean', hue='Column', data=df_final, palette='viridis')\n",
        "\n",
        "      # Adicionar t√≠tulos e r√≥tulos\n",
        "      plt.xlabel('Sensores')\n",
        "      plt.ylabel('M√©dia')\n",
        "      plt.title('M√©dia dos Sensores para o m√©todo LIME')\n",
        "\n",
        "      # Mostrar o gr√°fico\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "d5ImM-uC_UUc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GRAD-CAM**"
      ],
      "metadata": {
        "id": "am70VoG-Ptch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRAD_CAM_Method:\n",
        "  def __init__(self, features_name, api):\n",
        "    self.api = api\n",
        "    self.features_name = features_name\n",
        "\n",
        "  def run_GRAD_CAM_Method(self, test_data,sample_index, model, class_names, last_layer_name):\n",
        "      # Selecionar uma inst√¢ncia para visualiza√ß√£o\n",
        "      print(\"Running GRAD_CAM...\")\n",
        "      instancia = test_data[sample_index]  # Por exemplo, a primeira inst√¢ncia do conjunto de dados\n",
        "\n",
        "      # Preparar a inst√¢ncia para o modelo (adicionar uma dimens√£o extra se necess√°rio)\n",
        "      instancia = np.expand_dims(instancia, axis=0)\n",
        "\n",
        "      # Obter a sa√≠da do modelo para a inst√¢ncia selecionada\n",
        "      predicao = model.predict(instancia)\n",
        "\n",
        "      # Obter a classe prevista\n",
        "      classe_prevista = np.argmax(predicao[0])\n",
        "\n",
        "      # Obter o output do √∫ltimo layer convolucional\n",
        "      ultimo_conv_layer = model.get_layer(last_layer_name) #em alguns casos, isso pode mudar\n",
        "\n",
        "      # Criar um modelo para Grad-CAM\n",
        "      grad_model = tf.keras.models.Model(\n",
        "          [model.inputs],\n",
        "          [ultimo_conv_layer.output, model.output]\n",
        "      )\n",
        "\n",
        "      # Obter os gradientes da classe prevista em rela√ß√£o ao √∫ltimo layer convolucional\n",
        "      with tf.GradientTape() as tape:\n",
        "          conv_outputs, predictions = grad_model(instancia)\n",
        "          predictions = tf.convert_to_tensor(predictions)  # Converter para tensor\n",
        "\n",
        "          # Verificar o tamanho de predictions\n",
        "          num_classes = predictions.shape[-1]\n",
        "          if classe_prevista >= num_classes:\n",
        "              raise ValueError(f\"Classe prevista ({classe_prevista}) est√° fora dos limites (0 a {num_classes-1}).\")\n",
        "\n",
        "          # Garantir que predictions √© um tensor de float32\n",
        "          predictions = tf.cast(predictions, tf.float32)\n",
        "          print(predictions[0][0][classe_prevista])\n",
        "          loss = predictions[0][0][classe_prevista] #predictions[:, classe_prevista]\n",
        "\n",
        "      # Gradientes em rela√ß√£o √† sa√≠da do √∫ltimo layer convolucional\n",
        "      grads = tape.gradient(loss, conv_outputs)[0]\n",
        "\n",
        "      # M√©dia ponderada dos canais da sa√≠da do layer convolucional\n",
        "      pooled_grads = tf.reduce_mean(grads, axis=(0, 1))\n",
        "\n",
        "      # Multiplicar cada canal na sa√≠da do feature map pelo \"importance\" desse canal\n",
        "      heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs[0]), axis=-1)\n",
        "      # dataFrame = self.api.create_df_2(heatmap)\n",
        "      # df = self.api.get_most_important_features(dataFrame, 3)\n",
        "      return heatmap, grad_model, classe_prevista\n",
        "      # Normalizar o heatmap\n",
        "      # heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
        "      # heatmap\n",
        "  def summarize_df(self, df_results):\n",
        "      # Concatenar os DataFrames\n",
        "      df_final = pd.concat(df_results, ignore_index=True)\n",
        "\n",
        "      # Contar a frequ√™ncia de cada valor na coluna 'Coluna'\n",
        "      coluna_counts = df_final['Column'].value_counts()\n",
        "\n",
        "      # Filtrar os valores que aparecem mais de uma vez\n",
        "      repeated_sensors = coluna_counts[coluna_counts > 1].index\n",
        "\n",
        "      # Criar um novo DataFrame com as linhas que t√™m 'Coluna' repetida\n",
        "      df_final = df_final[df_final['Column'].isin(repeated_sensors)]\n",
        "\n",
        "      # Calcular a m√©dia das colunas repetidas\n",
        "      df_final = df_final.groupby('Column').mean().reset_index()\n",
        "\n",
        "      # Ordenar o DataFrame final pela coluna 'Coluna'\n",
        "      df_final = df_final.sort_values(by='Column')\n",
        "\n",
        "      return df_final\n",
        "\n",
        "  def run_mulltiple_GRAD_Method(self, test_data,sample_index, model, class_names,last_layer_name, REPEATS):\n",
        "      print(\"Running GRAD_CAM...\")\n",
        "      df_results = []\n",
        "      for i in range(REPEATS):\n",
        "          print(f\"Cicle: {i+1} of {REPEATS}...\")\n",
        "          heatmap, grad_model, classe_prevista = self.run_GRAD_CAM_Method(test_data=test_data, sample_index=sample_index, model=model, class_names=class_names,last_layer_name=last_layer_name)\n",
        "          dataFrame = self.api.create_df_2(heatmap)\n",
        "          df = self.api.get_most_important_features(dataFrame, 3)\n",
        "          df_results.append(df)\n",
        "      return self.summarize_df(df_results)\n",
        "\n",
        "  def plot_heatmap(self, df_grad):\n",
        "      # Criar o gr√°fico de barras\n",
        "      fig = px.bar(df_grad, x='Feature', y='Weight', color='Weight', color_continuous_scale='Viridis')\n",
        "\n",
        "      # Atualizar o layout para permitir zoom e melhor visualiza√ß√£o\n",
        "      fig.update_layout(\n",
        "          title='Gr√°fico de Barras dos Pesos das Caracter√≠sticas (Grad-CAM)',\n",
        "          xaxis=dict(title='Feature'),\n",
        "          yaxis=dict(title='Weight'),\n",
        "          autosize=True\n",
        "      )\n",
        "\n",
        "      # Mostrar o gr√°fico de barras\n",
        "      fig.show()\n",
        "\n",
        "  def plot_bar_chart(self, df_final):\n",
        "      # Configurar o estilo do Seaborn\n",
        "      sns.set(style=\"whitegrid\")\n",
        "\n",
        "      # Criar o gr√°fico de barras\n",
        "      plt.figure(figsize=(12, 8))\n",
        "      barplot = sns.barplot(x='Column', y='Mean', hue='Column', data=df_final, palette='viridis')\n",
        "\n",
        "      # Adicionar t√≠tulos e r√≥tulos\n",
        "      plt.xlabel('Sensores')\n",
        "      plt.ylabel('M√©dia')\n",
        "      plt.title('M√©dia dos Sensores para o m√©todo LIME')\n",
        "\n",
        "      # Mostrar o gr√°fico\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "zyP0mt07PwwZ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run all methods**"
      ],
      "metadata": {
        "id": "r6qUWzvFmLhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Run_methods:\n",
        "  def __init__(self, api):\n",
        "      self.api = api\n",
        "      self.LIME = LIME_Method(self.api)\n",
        "      self.SHAP = SHAP_Method(self.api.features_name, self.api)\n",
        "      self.GRAD = GRAD_CAM_Method(self.api.features_name, self.api)\n",
        "\n",
        "  def run_all_methods_mult(self, X_train, X_test,sample_index, model, class_names, npermutations,last_layer_name, REPEATS):\n",
        "\n",
        "      df_lime = self.LIME.run_mulltiple_LIME_Method(train_data=X_train, test_data=X_test, sample_index=SAMPLE_INDEX, model=model, class_names=class_names, REPEATS=REPEATS)\n",
        "      df_shap = self.SHAP.run_mulltiple_SHAP_Method(train_data=X_train, test_data=X_test, sample_index=sample_index, model=model, class_names=class_names, npermutations=npermutations, REPEATS=REPEATS)\n",
        "      df_GRAD = self.GRAD.run_mulltiple_GRAD_Method(test_data=X_test, sample_index=SAMPLE_INDEX, model=model,class_names=class_names, last_layer_name='conv1d_30', REPEATS=REPEATS)\n",
        "\n",
        "      df_final = self.api.get_features_in_common(df_LIME, df_shap, df_GRAD)\n",
        "      df_caounts = self.api.get_sensor_repeats(df_final)\n",
        "\n",
        "      return df_LIME, df_shap, df_GRAD, df_final, df_caounts\n",
        "\n",
        "  def run_2_methods_mult(self, X_train, X_test, sample_index, model, class_names, npermutations,last_layer_name, REPEATS):\n",
        "\n",
        "      df_LIME = self.LIME.run_mulltiple_LIME_Method(train_data=X_train, test_data=X_test, sample_index=SAMPLE_INDEX, model=model, class_names=class_names, REPEATS=REPEATS)\n",
        "      df_shap = pd.DataFrame(data={'Column':[0],'Mean':[0],'DataFrame':'SHAP'})\n",
        "      df_GRAD = self.GRAD.run_mulltiple_GRAD_Method(test_data=X_test, sample_index=SAMPLE_INDEX, model=model,class_names=class_names, last_layer_name='conv1d_30', REPEATS=REPEATS)\n",
        "\n",
        "      df_final = self.api.get_features_in_common(df_LIME, df_shap, df_GRAD)\n",
        "      df_caounts = self.api.get_sensor_repeats(df_final)\n",
        "\n",
        "      return df_LIME, df_shap, df_GRAD, df_final, df_caounts\n",
        "\n",
        "  def run_all_methods_once(self, X_train, X_test,sample_index, model, class_names, npermutations,last_layer_name):\n",
        "\n",
        "      # REPEAT = 4\n",
        "      # df_final = LIME.run_mulltiple_LIME_Method(train_data=X_train, test_data=X_test, sample_index=SAMPLE_INDEX, model=model, class_names=class_names, REPEATS=REPEAT)\n",
        "      # df_final\n",
        "      explanation, index, wiegths = self.LIME.run_LIME_Method(train_data=X_train, test_data=X_test, sample_index=sample_index, model=model, class_names=class_names)\n",
        "      dataFrame_1 = self.api.create_df(wiegths, index)\n",
        "      df_LIME = self.api.get_most_important_features(dataFrame_1, 3)\n",
        "\n",
        "      explainer,shap_values, feature_weights = self.SHAP.run_SHAP_Method(train_data=X_train, test_data=X_test, sample_index=sample_index, model=model, class_names=class_names, npermutations=npermutations)\n",
        "      dataFrame_2 = self.api.create_df_2(list_weights=feature_weights)\n",
        "      df_shap = self.api.get_most_important_features(dataFrame_2, 3)\n",
        "      # df_shap = pd.DataFrame(data={'Column':[0],'Mean':[0],'DataFrame':'SHAP'})\n",
        "      # heatmap, grad_model, classe_prevista = GRAD.run_GRAD_CAM_Method(test_data=X_test, sample_index=SAMPLE_INDEX, model=model,class_names=class_names, last_layer_name='conv1d_30')\n",
        "      heatmap, grad_model, classe_prevista = self.GRAD.run_GRAD_CAM_Method(test_data=X_test, sample_index=sample_index, model=model,class_names=class_names, last_layer_name=last_layer_name)\n",
        "      dataFrame_3 = self.api.create_df_2(heatmap)\n",
        "      df_GRAD = self.api.get_most_important_features(dataFrame_3, 3)\n",
        "\n",
        "      df_final = self.api.get_features_in_common(df_LIME, df_shap, df_GRAD)\n",
        "      df_caounts = self.api.get_sensor_repeats(df_final)\n",
        "\n",
        "      return df_LIME, df_shap, df_GRAD, df_final, df_caounts\n",
        "\n",
        "  def run_2_methods_once(self, X_train, X_test,sample_index, model, class_names, last_layer_name):\n",
        "\n",
        "      # REPEAT = 4\n",
        "      # df_final = LIME.run_mulltiple_LIME_Method(train_data=X_train, test_data=X_test, sample_index=SAMPLE_INDEX, model=model, class_names=class_names, REPEATS=REPEAT)\n",
        "      # df_final\n",
        "      explanation, index, wiegths = self.LIME.run_LIME_Method(train_data=X_train, test_data=X_test, sample_index=sample_index, model=model, class_names=class_names)\n",
        "      dataFrame_1 = self.api.create_df(wiegths, index)\n",
        "      df_LIME = self.api.get_most_important_features(dataFrame_1, 3)\n",
        "\n",
        "      df_shap = pd.DataFrame(data={'Column':[0],'Mean':[0],'DataFrame':'SHAP'})\n",
        "      # heatmap, grad_model, classe_prevista = GRAD.run_GRAD_CAM_Method(test_data=X_test, sample_index=SAMPLE_INDEX, model=model,class_names=class_names, last_layer_name='conv1d_30')\n",
        "\n",
        "      heatmap, grad_model, classe_prevista = self.GRAD.run_GRAD_CAM_Method(test_data=X_test, sample_index=sample_index, model=model,class_names=class_names, last_layer_name=last_layer_name)\n",
        "      dataFrame_3 = self.api.create_df_2(heatmap)\n",
        "      df_GRAD = self.api.get_most_important_features(dataFrame_3, 3)\n",
        "\n",
        "      df_final = self.api.get_features_in_common(df_LIME, df_shap, df_GRAD)\n",
        "      df_caounts = self.api.get_sensor_repeats(df_final)\n",
        "\n",
        "      return df_LIME, df_shap, df_GRAD, df_final, df_caounts\n",
        "\n",
        "  def plot_all_bar_charts(self,df_LIME, df_shap, df_GRAD):\n",
        "      self.LIME.plot_bar_chart(df_LIME)\n",
        "      if(df_shap.Mean.mean() !=0):\n",
        "        self.SHAP.plot_bar_chart(df_shap)\n",
        "\n",
        "      self.GRAD.plot_bar_chart(df_GRAD)\n",
        "\n",
        "  def print_result(self, df_caounts, db_name, query_fungi, query_sensor):\n",
        "      import sqlite3\n",
        "      conn = sqlite3.connect(db_name)\n",
        "      fungi_dict, fungi = self.api.get_dict_by_query(query_fungi,conn)\n",
        "      sensor_dict, sensor = self.api.get_dict_by_query(query_sensor,conn)\n",
        "\n",
        "      for sensor in df_caounts.Sensors:\n",
        "        #   print(sensor)\n",
        "          top_fungi = self.api.find_top_compatible_fungi(sensor,fungi_dict,sensor_dict)\n",
        "          print(f\"Os 3 fungos mais compat√≠veis com o sensor {sensor} s√£o:\")\n",
        "          for fungus, count, details in top_fungi:\n",
        "              print(f\"{fungus} com {count} categorias compat√≠veis:\")\n",
        "              for category, (sensor_count, fungus_count) in details.items():\n",
        "                  print(f\"  - {category}: Sensor ({sensor_count} vezes), Fungo ({fungus_count} vezes)\")\n",
        "              print()\n",
        "\n",
        "  def plot_bar_chart_all_methods(self, dataFrame):\n",
        "      sns.set(style=\"whitegrid\")\n",
        "      # Criando a figura com maior DPI e tamanho\n",
        "      plt.figure(figsize=(12, 8), dpi=200)\n",
        "\n",
        "      # Criando o gr√°fico de barras\n",
        "      sns.barplot(x='Column', y='Mean', hue='DataFrame', data=dataFrame, palette='viridis')\n",
        "\n",
        "      # Adicionando labels e t√≠tulo\n",
        "      plt.xlabel('Sensors', fontweight='bold')\n",
        "      plt.ylabel('Average values', fontweight='bold')\n",
        "      plt.title('DataFrame Sensors Averages')\n",
        "\n",
        "      # Rotacionando os nomes das colunas no eixo X\n",
        "      plt.xticks(rotation=45)\n",
        "\n",
        "      # Exibindo o gr√°fico\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "APZ7qZZFmOMy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Teste**"
      ],
      "metadata": {
        "id": "TgitoI7DR815"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# features_name = [\"TGS-826\",\"TGS-2611\",\"TGS-2603\",\"TGS-813\",\"TGS-822\",\"TGS-2602\",\"TGS-823\"]\n",
        "# class_names = ['Albicans', 'Glabrata', 'Haemulonii', 'Kodamaea_ohmeri', 'Krusei', 'Parapsilosis']\n",
        "# api = ExplainableAPI(features_name)\n",
        "# X_train, y_train = api.load_data(path=\"AllCandidas_TRAIN.csv\", sep=\",\")  # Replace with actual path and separator\n",
        "# X_test, y_test = api.load_data(path=\"AllCandidas_TEST.csv\", sep=\",\")  # Replace with actual path and separator\n",
        "# SAMPLE_INDEX = 19# amostra que ser√° selecionada do df de teste\n",
        "# model = tf.keras.models.load_model('/content/best_model.hdf5')\n",
        "\n",
        "# main = Run_methods(api)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMcju9OStDiV",
        "outputId": "c07bc7cc-0687-47c0-fd6d-be69548846c0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
            "  warnings.warn(\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        }
      ]
    }
  ]
}