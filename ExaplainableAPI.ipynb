{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AdaQttrzx8_6",
        "aygJH5C7_KcT",
        "jiuxnZsa_GDK",
        "2SXH8jT__Qph",
        "am70VoG-Ptch"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaellopes16/ExplainabilityEnsembleAPI/blob/main/ExaplainableAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install Libs**\n",
        "\n"
      ],
      "metadata": {
        "id": "AdaQttrzx8_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !jupyter nbconvert --to script ExplainableAPI.ipynb\n"
      ],
      "metadata": {
        "id": "76I9ZKEjPQ6e"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime==0.1.1.37"
      ],
      "metadata": {
        "id": "3YGmvoFh7qXn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3391fe87-6054-4f2a-cd32-9fb6f434626e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lime==0.1.1.37 in /usr/local/lib/python3.10/dist-packages (0.1.1.37)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lime==0.1.1.37) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime==0.1.1.37) (1.13.1)\n",
            "Requirement already satisfied: progressbar in /usr/local/lib/python3.10/dist-packages (from lime==0.1.1.37) (2.5)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime==0.1.1.37) (1.3.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime==0.1.1.37) (3.7.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime==0.1.1.37) (0.23.2)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime==0.1.1.37) (3.3)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime==0.1.1.37) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime==0.1.1.37) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime==0.1.1.37) (2024.8.28)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime==0.1.1.37) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime==0.1.1.37) (0.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime==0.1.1.37) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime==0.1.1.37) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime==0.1.1.37) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime==0.1.1.37) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime==0.1.1.37) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime==0.1.1.37) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime==0.1.1.37) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime==0.1.1.37) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lime==0.1.1.37) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap==0.46.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68ELXSZA_lED",
        "outputId": "d807f087-5ba0-4604-a155-98ac65654a7a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap==0.46.0 in /usr/local/lib/python3.10/dist-packages (0.46.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (1.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (2.1.4)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (4.66.5)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (24.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (0.0.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap==0.46.0) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap==0.46.0) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap==0.46.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap==0.46.0) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap==0.46.0) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap==0.46.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap==0.46.0) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap==0.46.0) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTS**"
      ],
      "metadata": {
        "id": "aygJH5C7_KcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: crie um classe em python chamada ExplainableAPI com o método load_data que recebe o caminho, o separador e retorna um x e um y\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "# from multipledispatch import dispatch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import lime.lime_tabular # !pip install lime==0.1.1.37\n",
        "import shap #!pip install shap==0.46.0"
      ],
      "metadata": {
        "id": "I8Cfdiu97kAd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GENERAL API**"
      ],
      "metadata": {
        "id": "pdStW9-5_MkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExplainableAPI:\n",
        "  QUERY_FUNGI = '''SELECT\n",
        "            f.name AS Fungi_Name,\n",
        "            v.VOC_Category,\n",
        "            COUNT(*) AS VOC_Count\n",
        "        FROM\n",
        "            FUNGI f\n",
        "        JOIN\n",
        "            FUNGI_VOC fv ON f.id = fv.FUNGI_ID\n",
        "        JOIN\n",
        "            VOCs v ON fv.VOC_ID = v.id_VOC\n",
        "        GROUP BY\n",
        "            f.name, v.VOC_Category\n",
        "        ORDER BY\n",
        "            f.name, v.VOC_Category,VOC_Count;\n",
        "        '''\n",
        "  QUERY_SENSOR = '''SELECT\n",
        "            f.name AS Sensor_Name,\n",
        "            v.VOC_Category,\n",
        "            COUNT(*) AS VOC_Count\n",
        "        FROM\n",
        "            SENSOR f\n",
        "        JOIN\n",
        "            SENSOR_VOC fv ON f.id = fv.SENSOR_ID\n",
        "        JOIN\n",
        "            VOCs v ON fv.VOC_ID = v.id_VOC\n",
        "        GROUP BY\n",
        "            f.name, v.VOC_Category\n",
        "        ORDER BY\n",
        "            f.name,VOC_Count, v.VOC_Category;\n",
        "        '''\n",
        "  def __init__(self, features_name):\n",
        "    self.features_name = features_name\n",
        "  def load_data(self, path, sep):\n",
        "      data = pd.read_csv(path, delimiter=sep,header=None)\n",
        "      print()\n",
        "      y = data.iloc[:, data.shape[1]-1].values\n",
        "      x = data.iloc[:,0: data.shape[1]-1].copy().values\n",
        "      return x, y\n",
        "\n",
        "  def create_df_2(self, list_weights):\n",
        "      df = pd.DataFrame({\n",
        "      'Feature': range(len(list_weights)),\n",
        "      'Weight': list_weights\n",
        "      })\n",
        "      return df\n",
        "\n",
        "  def get_predicted_class(self, sample, model):\n",
        "      instancia = np.expand_dims(sample, axis=0)\n",
        "      result = model.predict(instancia)\n",
        "      return np.argmax(result[0]) +1\n",
        "\n",
        "  def create_df(self, list_weights, features):\n",
        "      df = pd.DataFrame({\n",
        "      'Feature': features,\n",
        "      'Weight': list_weights\n",
        "      })\n",
        "      return df\n",
        "      #melhorar isso\n",
        "  def get_samples_from_db(self, X_train, y_train, predictedClass):\n",
        "      # Filtrar as instâncias pela classe desejada\n",
        "      indices_classe = np.where(y_train == predictedClass)[0]\n",
        "      X_classe = X_train[indices_classe]\n",
        "      print(\"X_classe.shape[0]\")\n",
        "      print(X_classe.shape[0])\n",
        "      # Selecionar 3 exemplos aleatórios das instâncias filtradas\n",
        "      amostras_aleatorias = X_classe[np.random.choice(X_classe.shape[0], 3, replace=False)]\n",
        "      # Converter cada exemplo selecionado em um DataFrame e armazenar em um array\n",
        "      dataframes = [pd.DataFrame(amostra.reshape(1, -1)) for amostra in amostras_aleatorias]\n",
        "      return dataframes\n",
        "\n",
        "  def plot_cycles(self,newDF):\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Plotando os dataframes em gráficos separados\n",
        "    for i, df in enumerate(newDF):\n",
        "        fig = plt.figure(figsize=[20,10])\n",
        "        plt.plot(df.T, label=f'Ciclo {i+1}')\n",
        "        plt.title(f'Gráfico de Linha do Ciclo {i+1}')\n",
        "        plt.xlabel('Índice')\n",
        "        plt.ylabel('Valores')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    # Mostrando o gráfico\n",
        "    plt.show()\n",
        "\n",
        "  def weight_by_feature(self, features_name, weight_list):\n",
        "      # Converte a lista de pesos em um array NumPy para facilitar a manipulação\n",
        "      weight_array = np.array(weight_list)\n",
        "\n",
        "      # Calcula o número de linhas que o DataFrame terá\n",
        "      num_rows = len(weight_list) // len(features_name)\n",
        "\n",
        "      # Redimensiona o array para ter 'num_rows' linhas e 'len(features_name)' colunas\n",
        "      reshaped_array = weight_array[:num_rows * len(features_name)].reshape(num_rows, len(features_name))\n",
        "\n",
        "      # Cria o DataFrame a partir do array redimensionado\n",
        "      df = pd.DataFrame(reshaped_array, columns=features_name)\n",
        "      return df\n",
        "\n",
        "  def get_final_result(self, wigths, feature):\n",
        "      df = self.create_df(wigths, feature)\n",
        "      self.ploat_heatmap(df)\n",
        "      self.ploat_bar(df)\n",
        "      df1 = self.weight_by_feature(self.features_name, df.Weight)\n",
        "      df1.max().plot(kind='bar')\n",
        "      df1.mean().plot(kind='bar')\n",
        "      return df1\n",
        "\n",
        "  def get_most_important_features(self, dataFrame, top_feature_number):\n",
        "\n",
        "      df = self.weight_by_feature(self.features_name, dataFrame.Weight)\n",
        "\n",
        "      newDf = df.mean().reset_index()\n",
        "      newDf.columns = ['Column', 'Mean']\n",
        "      newDf = newDf.sort_values(by='Mean', ascending=False)\n",
        "      top_features = newDf.head(top_feature_number)\n",
        "      return top_features\n",
        "\n",
        "  def concat_all_mothods(self, means_lime, means_shap, means_grad):\n",
        "      from sklearn.preprocessing import StandardScaler\n",
        "      scaler = StandardScaler()\n",
        "      # Calculando as médias das colunas\n",
        "      means_lime['Mean'] = scaler.fit_transform(means_lime[['Mean']])\n",
        "      means_shap['Mean'] = scaler.fit_transform(means_shap[['Mean']])\n",
        "      means_grad['Mean'] = scaler.fit_transform(means_grad[['Mean']])\n",
        "\n",
        "      # # Renomeando as colunas para facilitar a concatenação\n",
        "      # means_lime.columns = ['Column', 'Mean']\n",
        "      # means_shap.columns = ['Column', 'Mean']\n",
        "      # means_grad.columns = ['Column', 'Mean']\n",
        "    #   print(\"means_grad\")\n",
        "    #   print(means_grad)\n",
        "      # Adicionando uma coluna para identificar o dataframe\n",
        "      means_lime['DataFrame'] = 'LIME'\n",
        "      means_shap['DataFrame'] = 'SHAP'\n",
        "      means_grad['DataFrame'] = 'GRAD'\n",
        "\n",
        "      # Concatenando os dataframes\n",
        "      means_all = pd.concat([means_lime, means_shap, means_grad])\n",
        "      print(means_all)\n",
        "      # means_all['Mean'] = scaler.fit_transform(means_all[['Mean']])\n",
        "    #   self.plot_bar_chart_all_methods(means_all)\n",
        "      return means_all, means_lime, means_shap, means_grad\n",
        "\n",
        "  def get_top_features(self,means_lime,means_shap,means_grad,top_index ):\n",
        "      df_sorted_1 = means_lime.sort_values(by='Mean', ascending=False)\n",
        "      df_sorted_2 = means_shap.sort_values(by='Mean', ascending=False)\n",
        "      df_sorted_3 = means_grad.sort_values(by='Mean', ascending=False)\n",
        "\n",
        "      # Selecionar as três primeiras linhas\n",
        "      top_3_1 = df_sorted_1.head(top_index)\n",
        "      top_3_2 = df_sorted_2.head(top_index)\n",
        "      top_3_3 = df_sorted_3.head(top_index)\n",
        "\n",
        "      return top_3_1, top_3_2, top_3_3\n",
        "\n",
        "  def get_features_in_common(self, df1, df2, df3):\n",
        "      means_all, means_lime, means_shap, means_grad = self.concat_all_mothods(df1, df2, df3)\n",
        "      top_3_1, top_3_2, top_3_3 = self.get_top_features(means_lime, means_shap, means_grad, 3 )\n",
        "      # Adicionar uma coluna de peso a cada DataFrame. Para remover os pesos, comentar essas linhas e a de média ponderada com o [X]\n",
        "      # top_3_1['Peso'] = 0.25\n",
        "      # top_3_2['Peso'] = 0.35\n",
        "      # top_3_3['Peso'] = 0.40\n",
        "\n",
        "      df_concat = pd.concat([top_3_1, top_3_2, top_3_3])\n",
        "      # Contar a frequência de cada valor na coluna 'Coluna'\n",
        "      coluna_counts = df_concat['Column'].value_counts()\n",
        "      # Filtrar os valores que aparecem mais de uma vez\n",
        "      repeated_sensors = coluna_counts[coluna_counts > 1].index\n",
        "      # Criar um novo DataFrame com as linhas que têm 'Coluna' repetida\n",
        "      df_final = df_concat[df_concat['Column'].isin(repeated_sensors)]\n",
        "\n",
        "      # Calcular a média ponderada para cada valor na coluna 'Coluna' [X]\n",
        "      # df_final = df_final.groupby('Column').apply(lambda x: (x['Peso'] * x.drop(columns=['Peso'])).sum() / x['Peso'].sum())\n",
        "\n",
        "      df_final = df_final.sort_values(by='Column')\n",
        "      return df_final\n",
        "  def get_sensor_repeats(self, df_final):\n",
        "      coluna_counts = df_final['Column'].value_counts()\n",
        "      df_counts = pd.DataFrame({\n",
        "          'Sensors': coluna_counts.index,\n",
        "          'Repeats': coluna_counts.values\n",
        "      })\n",
        "      df_counts = df_counts.sort_values(by='Sensors').reset_index(drop=True)\n",
        "      return df_counts\n",
        "\n",
        "  def get_dict_by_query(self, query_sql, conn):\n",
        "      cursor = conn.cursor()\n",
        "      cursor.execute(query_sql)\n",
        "      conn.commit()\n",
        "      result = cursor.fetchall()\n",
        "      result_dict = {}\n",
        "      for data, category, count in result:\n",
        "          if data not in result_dict:\n",
        "              result_dict[data] = {}\n",
        "          result_dict[data][category] = count\n",
        "      return result_dict, result\n",
        "\n",
        "  # Função para verificar compatibilidade e retornar os 3 fungos mais compatíveis\n",
        "  def find_top_compatible_fungi(self, sensor,fungi_dict,sensor_dict ):\n",
        "      sensor_categories = sensor_dict.get(sensor, {})\n",
        "      compatibility = []\n",
        "\n",
        "      for fungus, categories in fungi_dict.items():\n",
        "          common_categories = set(sensor_categories.keys()).intersection(categories.keys())\n",
        "          if len(common_categories) >= 1:\n",
        "              common_details = {category: (sensor_categories[category], categories[category]) for category in common_categories}\n",
        "              compatibility.append((fungus, len(common_categories), common_details))\n",
        "\n",
        "      # Ordenar por número de categorias compatíveis (maior para menor)\n",
        "      compatibility.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "      # Ordenar as categorias compatíveis por quantidade de repetições (maior para menor)\n",
        "      for i in range(len(compatibility)):\n",
        "          compatibility[i] = (compatibility[i][0], compatibility[i][1], dict(sorted(compatibility[i][2].items(), key=lambda item: item[1][1], reverse=True)))\n",
        "\n",
        "      # Retornar os 3 fungos mais compatíveis\n",
        "      return compatibility[:3]\n",
        "\n",
        "  def plot_samples_db(self, data):\n",
        "    for i, cycle in enumerate(data):\n",
        "        a = np.array(cycle)[0]\n",
        "        new_df = self.group_sensor(features_name, a)\n",
        "        self.plot_chart_line_df(new_df)\n",
        "\n",
        "  def group_sensor(self, features_name, row_cycle):\n",
        "        # Converte a lista de pesos em um array NumPy para facilitar a manipulação\n",
        "        weight_array = np.array(row_cycle)\n",
        "        # Calcula o número de linhas que o DataFrame terá\n",
        "        num_rows = len(row_cycle) // len(features_name)\n",
        "        # print(len(row_cycle))\n",
        "        # print(len(features_name))\n",
        "        # print(num_rows)\n",
        "        # Redimensiona o array para ter 'num_rows' linhas e 'len(features_name)' colunas\n",
        "        reshaped_array = weight_array[:num_rows * len(features_name)].reshape(num_rows, len(features_name))\n",
        "\n",
        "        # Cria o DataFrame a partir do array redimensionado\n",
        "        df = pd.DataFrame(reshaped_array, columns=features_name)\n",
        "\n",
        "        return df\n",
        "  # Adicionando uma coluna de índice para o eixo X\n",
        "  def plot_chart_line_df(self, dataFrame):\n",
        "      dataFrame['Index'] = dataFrame.index\n",
        "      # Transformando o DataFrame para o formato longo\n",
        "      df_long = pd.melt(dataFrame, id_vars=['Index'], var_name='Sensor', value_name='Valor')\n",
        "      # Plotando o gráfico de linha\n",
        "      plt.figure(figsize=(14, 8))\n",
        "      sns.lineplot(data=df_long, x='Index', y='Valor', hue='Sensor')\n",
        "      plt.xlabel('Index')\n",
        "      plt.ylabel('Value')\n",
        "      plt.title('Line chart sensors')\n",
        "      plt.legend(title='Sensors')\n",
        "      plt.show()\n",
        "\n",
        "  def ploat_heatmap(self, dataFrame):\n",
        "      fig = px.density_heatmap(dataFrame, x='Feature', y='Weight', nbinsx=20, nbinsy=20, color_continuous_scale='Viridis')\n",
        "      # Atualizar o layout para permitir zoom e melhor visualização\n",
        "      fig.update_layout(\n",
        "          title='Dynamic Heatmap of Feature Weightss',\n",
        "          xaxis=dict(title='Feature'),\n",
        "          yaxis=dict(title='Weight'),\n",
        "          autosize=True\n",
        "      )\n",
        "      # Mostrar o mapa de calor\n",
        "      fig.show()\n",
        "\n",
        "  def ploat_bar(self, dataFrame):\n",
        "      fig = px.bar(dataFrame, x='Feature', y='Weight', color='Weight', color_continuous_scale='Viridis')\n",
        "      # Atualizar o layout para permitir zoom e melhor visualização\n",
        "      fig.update_layout(\n",
        "          title='Feature Weights Bar Chart',\n",
        "          xaxis=dict(title='Feature'),\n",
        "          yaxis=dict(title='Weight'),\n",
        "          autosize=True\n",
        "      )\n",
        "      # Mostrar o gráfico de barras\n",
        "      fig.show()"
      ],
      "metadata": {
        "id": "HvN5VPYyyLmR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LIME METHOD**"
      ],
      "metadata": {
        "id": "jiuxnZsa_GDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LIME_Method:\n",
        "  def __init__(self, api):\n",
        "    self.api = api\n",
        "    pass\n",
        "  def run_LIME_Method(self, train_data, test_data,sample_index, model, class_names):\n",
        "    print(\"Running LIME...\")\n",
        "    feature_names = np.array(range(train_data.shape[1]))\n",
        "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "        training_data=train_data,\n",
        "        feature_names=feature_names,\n",
        "        class_names=class_names,\n",
        "        discretize_continuous=False  # ou True, dependendo da natureza dos seus dados\n",
        "    )\n",
        "    num_features = train_data.shape[1]\n",
        "    instance = test_data[sample_index]\n",
        "    explanation = explainer.explain_instance(\n",
        "        instance,\n",
        "        model.predict,\n",
        "        num_features= num_features\n",
        "    )\n",
        "    # Print the explanation\n",
        "    # for feature, weight in explanation.as_list():\n",
        "    #     print(f'{feature}: {weight}')\n",
        "    feature , weigths = zip(*sorted(explanation.as_list()))\n",
        "    weigths_array = np.array(weigths)\n",
        "    # weigths_matriz = weigths_array.reshape(1, -1)  # Redimensiona para 1 linha e 'n' colunas\n",
        "    return explanation, feature , weigths_array\n",
        "\n",
        "  def run_mulltiple_LIME_Method(self, train_data, test_data,sample_index, model, class_names,REPEATS):\n",
        "      print(\"Running LIME...\")\n",
        "      df_results = []\n",
        "      for i in range(REPEATS):\n",
        "          print(f\"Cicle: {i+1} of {REPEATS}...\")\n",
        "          explanation, index, wiegths = self.run_LIME_Method(train_data=train_data, test_data=test_data, sample_index=sample_index, model=model, class_names=class_names)\n",
        "          dataFrame = self.api.create_df(wiegths, index)\n",
        "          df = self.api.get_most_important_features(dataFrame, 3)\n",
        "          df_results.append(df)\n",
        "      return self.summarize_df(df_results)\n",
        "\n",
        "  def summarize_df(self, df_results):\n",
        "      #Colocar isso em um método\n",
        "      df_final = pd.concat(df_results, ignore_index=True)\n",
        "      # Contar a frequência de cada valor na coluna 'Coluna'\n",
        "      coluna_counts = df_final['Column'].value_counts()\n",
        "\n",
        "      # Filtrar os valores que aparecem mais de uma vez\n",
        "      repeated_sensors = coluna_counts[coluna_counts > 1].index\n",
        "\n",
        "      # Criar um novo DataFrame com as linhas que têm 'Coluna' repetida\n",
        "      df_final = df_final[df_final['Column'].isin(repeated_sensors)]\n",
        "\n",
        "      # Calcular a média das colunas repetidas\n",
        "      df_final = df_final.groupby('Column').mean().reset_index()\n",
        "\n",
        "      df_final = df_final.sort_values(by='Column')\n",
        "      return df_final\n",
        "\n",
        "  def plot_bar_chart(self, df_final):\n",
        "      # Configurar o estilo do Seaborn\n",
        "      sns.set(style=\"whitegrid\")\n",
        "\n",
        "      # Criar o gráfico de barras\n",
        "      plt.figure(figsize=(12, 8))\n",
        "      barplot = sns.barplot(x='Column', y='Mean', hue='Column', data=df_final, palette='viridis')\n",
        "\n",
        "      # Adicionar títulos e rótulos\n",
        "      plt.xlabel('Sensores')\n",
        "      plt.ylabel('Média')\n",
        "      plt.title('Média dos Sensores para o método LIME')\n",
        "\n",
        "      # Mostrar o gráfico\n",
        "      plt.show()\n",
        "\n",
        "  def plot_class_proba(self, class_name, explanation):\n",
        "      # Seus valores de probabilidade\n",
        "      probabilidades = explanation.predict_proba\n",
        "      # Nomes das classes para o eixo x do gráfico\n",
        "      plt.figure(figsize=(10, 7))  # Ajuste o tamanho conforme necessário\n",
        "      # Criar o gráfico de barras\n",
        "      plt.bar(class_name, probabilidades)\n",
        "\n",
        "      # Adicionar título e rótulos aos eixos\n",
        "      plt.title('Probabilidades das Classes')\n",
        "      plt.xlabel('Classes')\n",
        "      plt.ylabel('Probabilidade')\n",
        "\n",
        "      # Mostrar o gráfico\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "-3nY0q2l5-10"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df1 = api.get_final_result(wieghts,index)"
      ],
      "metadata": {
        "id": "awhbAXd0m2w_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SHAP METHOD**"
      ],
      "metadata": {
        "id": "2SXH8jT__Qph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SHAP_Method:\n",
        "  def __init__(self, features_name, api):\n",
        "    self.api = api\n",
        "    self.features_name = features_name\n",
        "  def run_SHAP_Method(self, train_data, test_data,sample_index, model, class_names, npermutations):\n",
        "    # explainer = shap.KernelExplainer(model.predict, train_data)\n",
        "    print(\"Running SHAP...\")\n",
        "    explainer = shap.PermutationExplainer(model.predict, train_data)\n",
        "    shap_values = explainer.shap_values(test_data[sample_index-1:sample_index], npermutations=npermutations)\n",
        "\n",
        "    predicted_classes, main_class_predicted = self.get_model_results(model, test_data,sample_index )\n",
        "\n",
        "    feature_weights = shap_values[:,:,main_class_predicted][0]  # Sum weights across classes #max_index\n",
        "    # culture_time_weigth = np.mean(feature_weights[0][train_data.shape[1]])\n",
        "    # mean_array = np.mean(feature_weights, axis=2)\n",
        "    # mean_array = mean_array[0][:-1]\n",
        "    # df2 = api.weight_by_feature(self.features_name, mean_array,culture_time_weigth)\n",
        "    return explainer,shap_values, feature_weights\n",
        "  def run_mulltiple_SHAP_Method(self, train_data, test_data,sample_index, model, class_names, npermutations, REPEATS):\n",
        "      print(\"Running SHAP...\")\n",
        "      df_results = []\n",
        "      for i in range(REPEATS):\n",
        "          print(f\"Cicle: {i+1} of {REPEATS}...\")\n",
        "          explanation, index, wieghts = self.run_SHAP_Method(train_data=train_data, test_data=test_data, sample_index=sample_index, model=model, class_names=class_names, npermutations=npermutations)\n",
        "          dataFrame = self.api.create_df_2(wieghts)\n",
        "          df = self.api.get_most_important_features(dataFrame, 3)\n",
        "          df_results.append(df)\n",
        "      return self.summarize_df(df_results)\n",
        "\n",
        "  def get_model_results(self, model, test_data, sample_index):\n",
        "      predicted_classes = model.predict(test_data[sample_index-1:sample_index])\n",
        "      main_class_predicted = np.argmax(predicted_classes)\n",
        "      return predicted_classes, main_class_predicted\n",
        "\n",
        "  def summarize_df(self, df_results):\n",
        "      #Colocar isso em um método\n",
        "      df_final = pd.concat(df_results, ignore_index=True)\n",
        "      # Contar a frequência de cada valor na coluna 'Coluna'\n",
        "      coluna_counts = df_final['Column'].value_counts()\n",
        "\n",
        "      # Filtrar os valores que aparecem mais de uma vez\n",
        "      repeated_sensors = coluna_counts[coluna_counts > 1].index\n",
        "\n",
        "      # Criar um novo DataFrame com as linhas que têm 'Coluna' repetida\n",
        "      df_final = df_final[df_final['Column'].isin(repeated_sensors)]\n",
        "\n",
        "      df_final = df_final.sort_values(by='Column')\n",
        "      return df_final\n",
        "\n",
        "  def plot_bar_chart(self, df_final):\n",
        "      # Configurar o estilo do Seaborn\n",
        "      sns.set(style=\"whitegrid\")\n",
        "\n",
        "      # Criar o gráfico de barras\n",
        "      plt.figure(figsize=(12, 8))\n",
        "      barplot = sns.barplot(x='Column', y='Mean', hue='Column', data=df_final, palette='viridis')\n",
        "\n",
        "      # Adicionar títulos e rótulos\n",
        "      plt.xlabel('Sensores')\n",
        "      plt.ylabel('Média')\n",
        "      plt.title('Média dos Sensores para o método LIME')\n",
        "\n",
        "      # Mostrar o gráfico\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "d5ImM-uC_UUc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GRAD-CAM**"
      ],
      "metadata": {
        "id": "am70VoG-Ptch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRAD_CAM_Method:\n",
        "  def __init__(self, features_name, api):\n",
        "    self.api = api\n",
        "    self.features_name = features_name\n",
        "\n",
        "  def run_GRAD_CAM_Method(self, test_data,sample_index, model, class_names, last_layer_name):\n",
        "      # Selecionar uma instância para visualização\n",
        "      print(\"Running GRAD_CAM...\")\n",
        "      instancia = test_data[sample_index]  # Por exemplo, a primeira instância do conjunto de dados\n",
        "\n",
        "      # Preparar a instância para o modelo (adicionar uma dimensão extra se necessário)\n",
        "      instancia = np.expand_dims(instancia, axis=0)\n",
        "\n",
        "      # Obter a saída do modelo para a instância selecionada\n",
        "      predicao = model.predict(instancia)\n",
        "\n",
        "      # Obter a classe prevista\n",
        "      classe_prevista = np.argmax(predicao[0])\n",
        "\n",
        "      # Obter o output do último layer convolucional\n",
        "      ultimo_conv_layer = model.get_layer(last_layer_name) #em alguns casos, isso pode mudar\n",
        "\n",
        "      # Criar um modelo para Grad-CAM\n",
        "      grad_model = tf.keras.models.Model(\n",
        "          [model.inputs],\n",
        "          [ultimo_conv_layer.output, model.output]\n",
        "      )\n",
        "\n",
        "      # Obter os gradientes da classe prevista em relação ao último layer convolucional\n",
        "      with tf.GradientTape() as tape:\n",
        "          conv_outputs, predictions = grad_model(instancia)\n",
        "          predictions = tf.convert_to_tensor(predictions)  # Converter para tensor\n",
        "\n",
        "          # Verificar o tamanho de predictions\n",
        "          num_classes = predictions.shape[-1]\n",
        "          if classe_prevista >= num_classes:\n",
        "              raise ValueError(f\"Classe prevista ({classe_prevista}) está fora dos limites (0 a {num_classes-1}).\")\n",
        "\n",
        "          # Garantir que predictions é um tensor de float32\n",
        "          predictions = tf.cast(predictions, tf.float32)\n",
        "          print(predictions[0][0][classe_prevista])\n",
        "          loss = predictions[0][0][classe_prevista] #predictions[:, classe_prevista]\n",
        "\n",
        "      # Gradientes em relação à saída do último layer convolucional\n",
        "      grads = tape.gradient(loss, conv_outputs)[0]\n",
        "\n",
        "      # Média ponderada dos canais da saída do layer convolucional\n",
        "      pooled_grads = tf.reduce_mean(grads, axis=(0, 1))\n",
        "\n",
        "      # Multiplicar cada canal na saída do feature map pelo \"importance\" desse canal\n",
        "      heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs[0]), axis=-1)\n",
        "      # dataFrame = self.api.create_df_2(heatmap)\n",
        "      # df = self.api.get_most_important_features(dataFrame, 3)\n",
        "      return heatmap, grad_model, classe_prevista\n",
        "      # Normalizar o heatmap\n",
        "      # heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
        "      # heatmap\n",
        "  def summarize_df(self, df_results):\n",
        "      # Concatenar os DataFrames\n",
        "      df_final = pd.concat(df_results, ignore_index=True)\n",
        "\n",
        "      # Contar a frequência de cada valor na coluna 'Coluna'\n",
        "      coluna_counts = df_final['Column'].value_counts()\n",
        "\n",
        "      # Filtrar os valores que aparecem mais de uma vez\n",
        "      repeated_sensors = coluna_counts[coluna_counts > 1].index\n",
        "\n",
        "      # Criar um novo DataFrame com as linhas que têm 'Coluna' repetida\n",
        "      df_final = df_final[df_final['Column'].isin(repeated_sensors)]\n",
        "\n",
        "      # Calcular a média das colunas repetidas\n",
        "      df_final = df_final.groupby('Column').mean().reset_index()\n",
        "\n",
        "      # Ordenar o DataFrame final pela coluna 'Coluna'\n",
        "      df_final = df_final.sort_values(by='Column')\n",
        "\n",
        "      return df_final\n",
        "\n",
        "  def run_mulltiple_GRAD_Method(self, test_data,sample_index, model, class_names,last_layer_name, REPEATS):\n",
        "      print(\"Running GRAD_CAM...\")\n",
        "      df_results = []\n",
        "      for i in range(REPEATS):\n",
        "          print(f\"Cicle: {i+1} of {REPEATS}...\")\n",
        "          heatmap, grad_model, classe_prevista = self.run_GRAD_CAM_Method(test_data=test_data, sample_index=sample_index, model=model, class_names=class_names,last_layer_name=last_layer_name)\n",
        "          dataFrame = self.api.create_df_2(heatmap)\n",
        "          df = self.api.get_most_important_features(dataFrame, 3)\n",
        "          df_results.append(df)\n",
        "      return self.summarize_df(df_results)\n",
        "\n",
        "  def plot_heatmap(self, df_grad):\n",
        "      # Criar o gráfico de barras\n",
        "      fig = px.bar(df_grad, x='Feature', y='Weight', color='Weight', color_continuous_scale='Viridis')\n",
        "\n",
        "      # Atualizar o layout para permitir zoom e melhor visualização\n",
        "      fig.update_layout(\n",
        "          title='Gráfico de Barras dos Pesos das Características (Grad-CAM)',\n",
        "          xaxis=dict(title='Feature'),\n",
        "          yaxis=dict(title='Weight'),\n",
        "          autosize=True\n",
        "      )\n",
        "\n",
        "      # Mostrar o gráfico de barras\n",
        "      fig.show()\n",
        "\n",
        "  def plot_bar_chart(self, df_final):\n",
        "      # Configurar o estilo do Seaborn\n",
        "      sns.set(style=\"whitegrid\")\n",
        "\n",
        "      # Criar o gráfico de barras\n",
        "      plt.figure(figsize=(12, 8))\n",
        "      barplot = sns.barplot(x='Column', y='Mean', hue='Column', data=df_final, palette='viridis')\n",
        "\n",
        "      # Adicionar títulos e rótulos\n",
        "      plt.xlabel('Sensores')\n",
        "      plt.ylabel('Média')\n",
        "      plt.title('Média dos Sensores para o método LIME')\n",
        "\n",
        "      # Mostrar o gráfico\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "zyP0mt07PwwZ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run all methods**"
      ],
      "metadata": {
        "id": "r6qUWzvFmLhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Run_methods:\n",
        "  def __init__(self, api):\n",
        "      self.api = api\n",
        "      self.LIME = LIME_Method(self.api)\n",
        "      self.SHAP = SHAP_Method(self.api.features_name, self.api)\n",
        "      self.GRAD = GRAD_CAM_Method(self.api.features_name, self.api)\n",
        "\n",
        "  def run_all_methods_mult(self, X_train, X_test,sample_index, model, class_names, npermutations,last_layer_name, REPEATS):\n",
        "\n",
        "      df_lime = self.LIME.run_mulltiple_LIME_Method(train_data=X_train, test_data=X_test, sample_index=SAMPLE_INDEX, model=model, class_names=class_names, REPEATS=REPEATS)\n",
        "      df_shap = self.SHAP.run_mulltiple_SHAP_Method(train_data=X_train, test_data=X_test, sample_index=sample_index, model=model, class_names=class_names, npermutations=npermutations, REPEATS=REPEATS)\n",
        "      df_GRAD = self.GRAD.run_mulltiple_GRAD_Method(test_data=X_test, sample_index=SAMPLE_INDEX, model=model,class_names=class_names, last_layer_name='conv1d_30', REPEATS=REPEATS)\n",
        "\n",
        "      df_final = self.api.get_features_in_common(df_LIME, df_shap, df_GRAD)\n",
        "      df_caounts = self.api.get_sensor_repeats(df_final)\n",
        "\n",
        "      return df_LIME, df_shap, df_GRAD, df_final, df_caounts\n",
        "\n",
        "  def run_2_methods_mult(self, X_train, X_test, sample_index, model, class_names, npermutations,last_layer_name, REPEATS):\n",
        "\n",
        "      df_LIME = self.LIME.run_mulltiple_LIME_Method(train_data=X_train, test_data=X_test, sample_index=SAMPLE_INDEX, model=model, class_names=class_names, REPEATS=REPEATS)\n",
        "      df_shap = pd.DataFrame(data={'Column':[0],'Mean':[0],'DataFrame':'SHAP'})\n",
        "      df_GRAD = self.GRAD.run_mulltiple_GRAD_Method(test_data=X_test, sample_index=SAMPLE_INDEX, model=model,class_names=class_names, last_layer_name='conv1d_30', REPEATS=REPEATS)\n",
        "\n",
        "      df_final = self.api.get_features_in_common(df_LIME, df_shap, df_GRAD)\n",
        "      df_caounts = self.api.get_sensor_repeats(df_final)\n",
        "\n",
        "      return df_LIME, df_shap, df_GRAD, df_final, df_caounts\n",
        "\n",
        "  def run_all_methods_once(self, X_train, X_test,sample_index, model, class_names, npermutations,last_layer_name):\n",
        "\n",
        "      # REPEAT = 4\n",
        "      # df_final = LIME.run_mulltiple_LIME_Method(train_data=X_train, test_data=X_test, sample_index=SAMPLE_INDEX, model=model, class_names=class_names, REPEATS=REPEAT)\n",
        "      # df_final\n",
        "      explanation, index, wiegths = self.LIME.run_LIME_Method(train_data=X_train, test_data=X_test, sample_index=sample_index, model=model, class_names=class_names)\n",
        "      dataFrame_1 = self.api.create_df(wiegths, index)\n",
        "      df_LIME = self.api.get_most_important_features(dataFrame_1, 3)\n",
        "\n",
        "      explainer,shap_values, feature_weights = self.SHAP.run_SHAP_Method(train_data=X_train, test_data=X_test, sample_index=sample_index, model=model, class_names=class_names, npermutations=npermutations)\n",
        "      dataFrame_2 = self.api.create_df_2(list_weights=feature_weights)\n",
        "      df_shap = self.api.get_most_important_features(dataFrame_2, 3)\n",
        "      # df_shap = pd.DataFrame(data={'Column':[0],'Mean':[0],'DataFrame':'SHAP'})\n",
        "      # heatmap, grad_model, classe_prevista = GRAD.run_GRAD_CAM_Method(test_data=X_test, sample_index=SAMPLE_INDEX, model=model,class_names=class_names, last_layer_name='conv1d_30')\n",
        "      heatmap, grad_model, classe_prevista = self.GRAD.run_GRAD_CAM_Method(test_data=X_test, sample_index=sample_index, model=model,class_names=class_names, last_layer_name=last_layer_name)\n",
        "      dataFrame_3 = self.api.create_df_2(heatmap)\n",
        "      df_GRAD = self.api.get_most_important_features(dataFrame_3, 3)\n",
        "\n",
        "      df_final = self.api.get_features_in_common(df_LIME, df_shap, df_GRAD)\n",
        "      df_caounts = self.api.get_sensor_repeats(df_final)\n",
        "\n",
        "      return df_LIME, df_shap, df_GRAD, df_final, df_caounts\n",
        "\n",
        "  def run_2_methods_once(self, X_train, X_test,sample_index, model, class_names, last_layer_name):\n",
        "\n",
        "      # REPEAT = 4\n",
        "      # df_final = LIME.run_mulltiple_LIME_Method(train_data=X_train, test_data=X_test, sample_index=SAMPLE_INDEX, model=model, class_names=class_names, REPEATS=REPEAT)\n",
        "      # df_final\n",
        "      explanation, index, wiegths = self.LIME.run_LIME_Method(train_data=X_train, test_data=X_test, sample_index=sample_index, model=model, class_names=class_names)\n",
        "      dataFrame_1 = self.api.create_df(wiegths, index)\n",
        "      df_LIME = self.api.get_most_important_features(dataFrame_1, 3)\n",
        "\n",
        "      df_shap = pd.DataFrame(data={'Column':[0],'Mean':[0],'DataFrame':'SHAP'})\n",
        "      # heatmap, grad_model, classe_prevista = GRAD.run_GRAD_CAM_Method(test_data=X_test, sample_index=SAMPLE_INDEX, model=model,class_names=class_names, last_layer_name='conv1d_30')\n",
        "\n",
        "      heatmap, grad_model, classe_prevista = self.GRAD.run_GRAD_CAM_Method(test_data=X_test, sample_index=sample_index, model=model,class_names=class_names, last_layer_name=last_layer_name)\n",
        "      dataFrame_3 = self.api.create_df_2(heatmap)\n",
        "      df_GRAD = self.api.get_most_important_features(dataFrame_3, 3)\n",
        "\n",
        "      df_final = self.api.get_features_in_common(df_LIME, df_shap, df_GRAD)\n",
        "      df_caounts = self.api.get_sensor_repeats(df_final)\n",
        "\n",
        "      return df_LIME, df_shap, df_GRAD, df_final, df_caounts\n",
        "\n",
        "  def plot_all_bar_charts(self,df_LIME, df_shap, df_GRAD):\n",
        "      self.LIME.plot_bar_chart(df_LIME)\n",
        "      if(df_shap.Mean.mean() !=0):\n",
        "        self.SHAP.plot_bar_chart(df_shap)\n",
        "\n",
        "      self.GRAD.plot_bar_chart(df_GRAD)\n",
        "\n",
        "  def print_result(self, df_caounts, db_name, query_fungi, query_sensor):\n",
        "      import sqlite3\n",
        "      conn = sqlite3.connect(db_name)\n",
        "      fungi_dict, fungi = self.api.get_dict_by_query(query_fungi,conn)\n",
        "      sensor_dict, sensor = self.api.get_dict_by_query(query_sensor,conn)\n",
        "\n",
        "      for sensor in df_caounts.Sensors:\n",
        "        #   print(sensor)\n",
        "          top_fungi = self.api.find_top_compatible_fungi(sensor,fungi_dict,sensor_dict)\n",
        "          print(f\"Os 3 fungos mais compatíveis com o sensor {sensor} são:\")\n",
        "          for fungus, count, details in top_fungi:\n",
        "              print(f\"{fungus} com {count} categorias compatíveis:\")\n",
        "              for category, (sensor_count, fungus_count) in details.items():\n",
        "                  print(f\"  - {category}: Sensor ({sensor_count} vezes), Fungo ({fungus_count} vezes)\")\n",
        "              print()\n",
        "\n",
        "  def plot_bar_chart_all_methods(self, dataFrame):\n",
        "      sns.set(style=\"whitegrid\")\n",
        "      # Criando a figura com maior DPI e tamanho\n",
        "      plt.figure(figsize=(12, 8), dpi=200)\n",
        "\n",
        "      # Criando o gráfico de barras\n",
        "      sns.barplot(x='Column', y='Mean', hue='DataFrame', data=dataFrame, palette='viridis')\n",
        "\n",
        "      # Adicionando labels e título\n",
        "      plt.xlabel('Sensors', fontweight='bold')\n",
        "      plt.ylabel('Average values', fontweight='bold')\n",
        "      plt.title('DataFrame Sensors Averages')\n",
        "\n",
        "      # Rotacionando os nomes das colunas no eixo X\n",
        "      plt.xticks(rotation=45)\n",
        "\n",
        "      # Exibindo o gráfico\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "APZ7qZZFmOMy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Teste**"
      ],
      "metadata": {
        "id": "TgitoI7DR815"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# features_name = [\"TGS-826\",\"TGS-2611\",\"TGS-2603\",\"TGS-813\",\"TGS-822\",\"TGS-2602\",\"TGS-823\"]\n",
        "# class_names = ['Albicans', 'Glabrata', 'Haemulonii', 'Kodamaea_ohmeri', 'Krusei', 'Parapsilosis']\n",
        "# api = ExplainableAPI(features_name)\n",
        "# X_train, y_train = api.load_data(path=\"AllCandidas_TRAIN.csv\", sep=\",\")  # Replace with actual path and separator\n",
        "# X_test, y_test = api.load_data(path=\"AllCandidas_TEST.csv\", sep=\",\")  # Replace with actual path and separator\n",
        "# SAMPLE_INDEX = 19# amostra que será selecionada do df de teste\n",
        "# model = tf.keras.models.load_model('/content/best_model.hdf5')\n",
        "\n",
        "# main = Run_methods(api)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMcju9OStDiV",
        "outputId": "c07bc7cc-0687-47c0-fd6d-be69548846c0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
            "  warnings.warn(\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        }
      ]
    }
  ]
}